# PRISM — Quantization Quality Estimation
# Identity Regime: W = I, same architecture, different precision.
#
# Quantisation backends (mix freely in quantization_bits):
#   No prefix  → GGUF file from proxy.model repo   (e.g. Q8_0, Q4_K_M)
#   bnb:       → bitsandbytes from target.model     (e.g. bnb:nf4, bnb:int8)
#   gptq:      → pre-quantised GPTQ model           (e.g. gptq:TheBloke/Llama-2-7B-GPTQ)
#               optionally with branch: gptq:REPO@REVISION
#
# Supported bnb: tags: int8, nf4, fp4, nf4-dq
#
# Usage (Llama-2-7b, default):
#   python run.py --config configs/quantization.yaml
#
# Usage (mixed GGUF + bitsandbytes + GPTQ):
#   python run.py --config configs/quantization.yaml \
#       'proxy.quantization_bits=[Q8_0,Q4_K_M,bnb:nf4,gptq:TheBloke/Llama-2-7B-GPTQ]'
#
# Usage (GPTQ with specific branch):
#   python run.py --config configs/quantization.yaml \
#       'proxy.quantization_bits=[gptq:TheBloke/Llama-2-7B-GPTQ@gptq-4bit-32g-actorder_True]'
#
# Usage (Qwen3-8B with NF4 only — no GGUF repo needed):
#   python run.py --config configs/quantization.yaml \
#       target.model=Qwen/Qwen3-8B \
#       'proxy.quantization_bits=[bnb:int8,bnb:nf4,bnb:fp4]' \
#       data.max_length=512
#
# GGUF filename template (auto-derived from proxy.model):
#   TheBloke repos  → {stem}.{quant}.gguf   (lowercase, dot separator)
#   Official repos  → {Stem}-{quant}.gguf   (original case, dash separator)
#   Override with proxy.gguf_template if the auto-derivation is wrong.

experiment: quantization

target:
  model: NousResearch/Llama-2-7b-hf
  extractor: llm

proxy:
  model: TheBloke/Llama-2-7b-GGUF
  extractor: llm
  # gguf_template: "llama-2-7b.{quant}.gguf"   # optional: override auto-derived
  quantization_bits:
    - Q8_0
    - Q6_K
    - Q5_K_M
    - Q4_K_M
    - Q3_K_M
    - Q2_K

data:
  task: c4
  split: test
  num_samples: 128
  batch_size: 4
  max_length: 2048

alignment:
  type: identity
  scale_absorbed: false   # true → ε_feat = √(2(1−Ω)), scale folded into head

output:
  dir: ./results/quantization
  save_features: true

device: cuda
seed: 42
