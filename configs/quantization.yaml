# PRISM — Quantization Quality Estimation
# Identity Regime: W = I, same architecture, different precision.
#
# Usage (Llama-2-7b, default):
#   python run.py --config configs/quantization.yaml
#
# Usage (Mistral-7B, CLI override):
#   python run.py --config configs/quantization.yaml \
#       target.model=mistralai/Mistral-7B-v0.1 \
#       proxy.model=TheBloke/Mistral-7B-v0.1-GGUF
#
# GGUF filename convention  (TheBloke repos):
#   {gguf_stem}.{QUANT}.gguf
#   gguf_stem is auto-derived from proxy.model by stripping the org prefix,
#   removing the "-GGUF" suffix, and lowercasing.
#   Override with proxy.gguf_stem if the auto-derivation is wrong.
#
# Examples:
#   TheBloke/Llama-2-7b-GGUF      → stem: llama-2-7b
#   TheBloke/Mistral-7B-v0.1-GGUF → stem: mistral-7b-v0.1
#   TheBloke/CodeLlama-7B-GGUF    → stem: codellama-7b

experiment: quantization

target:
  model: NousResearch/Llama-2-7b-hf
  extractor: llm

proxy:
  model: TheBloke/Llama-2-7b-GGUF
  extractor: llm
  # gguf_stem: llama-2-7b   # optional: override auto-derived stem
  quantization_bits:
    - Q8_0
    - Q6_K
    - Q5_K_M
    - Q4_K_M
    - Q3_K_M
    - Q2_K

data:
  task: c4
  split: test
  num_samples: 128
  batch_size: 4
  max_length: 2048

alignment:
  type: identity
  scale_absorbed: false   # true → ε_feat = √(2(1−Ω)), scale folded into head

output:
  dir: ./results/quantization
  save_features: true

device: cuda
seed: 42
