# PRISM — Quantization Quality Estimation
# Identity Regime: W = I, same architecture, different precision.
#
# Usage (Llama-2-7b, default):
#   python run.py --config configs/quantization.yaml
#
# Usage (Mistral-7B, CLI override — TheBloke repo, auto-derived template):
#   python run.py --config configs/quantization.yaml \
#       target.model=mistralai/Mistral-7B-v0.1 \
#       proxy.model=TheBloke/Mistral-7B-v0.1-GGUF
#
# Usage (Qwen3-8B, CLI override — official repo, auto-derived template):
#   python run.py --config configs/quantization.yaml \
#       target.model=Qwen/Qwen3-8B \
#       proxy.model=Qwen/Qwen3-8B-GGUF \
#       'proxy.quantization_bits=[Q8_0,Q6_K,Q5_K_M,Q4_K_M]'
#
# GGUF filename template (auto-derived from proxy.model):
#   TheBloke repos  → {stem}.{quant}.gguf   (lowercase, dot separator)
#   Official repos  → {Stem}-{quant}.gguf   (original case, dash separator)
#   Override with proxy.gguf_template if the auto-derivation is wrong.
#
# Auto-derivation examples:
#   TheBloke/Llama-2-7b-GGUF      → llama-2-7b.{quant}.gguf
#   TheBloke/Mistral-7B-v0.1-GGUF → mistral-7b-v0.1.{quant}.gguf
#   Qwen/Qwen3-8B-GGUF            → Qwen3-8B-{quant}.gguf

experiment: quantization

target:
  model: NousResearch/Llama-2-7b-hf
  extractor: llm

proxy:
  model: TheBloke/Llama-2-7b-GGUF
  extractor: llm
  # gguf_template: "llama-2-7b.{quant}.gguf"   # optional: override auto-derived
  quantization_bits:
    - Q8_0
    - Q6_K
    - Q5_K_M
    - Q4_K_M
    - Q3_K_M
    - Q2_K

data:
  task: c4
  split: test
  num_samples: 128
  batch_size: 4
  max_length: 2048

alignment:
  type: identity
  scale_absorbed: false   # true → ε_feat = √(2(1−Ω)), scale folded into head

output:
  dir: ./results/quantization
  save_features: true

device: cuda
seed: 42
